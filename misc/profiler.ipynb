{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from config import *\n",
    "import wandb\n",
    "import torch\n",
    "from typing import Dict\n",
    "from losses.losses import build_loss_fn\n",
    "from optimizers.optimizers import build_optimizer\n",
    "from train_scripts.train_functions import train_val\n",
    "from architectures.leavisnet_v3 import build_model\n",
    "\n",
    "# from datasets.build_dataset import build_architecture, build_dataloader\n",
    "\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_test_model(None)\n",
    "with torch.cuda.device(0):\n",
    "    net = model\n",
    "    macs, params = get_model_complexity_info(\n",
    "        net, (3, 224, 224), as_strings=True, print_per_layer_stat=True, verbose=True\n",
    "    )\n",
    "    print(\"{:<30}  {:<8}\".format(\"Computational complexity: \", macs))\n",
    "    print(\"{:<30}  {:<8}\".format(\"Number of parameters: \", params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_test_model(None).cpu()\n",
    "inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"\\nmodel parameter count = \", pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_inference(\n",
    "    model, sample_input, batch_size=1, use_amp=False, device=0\n",
    "):\n",
    "    \"\"\"Predict the maximum memory usage of the model.\n",
    "    Args:\n",
    "        optimizer_type (Type): the class name of the optimizer to instantiate\n",
    "        model (nn.Module): the neural network model\n",
    "        sample_input (torch.Tensor): A sample input to the network. It should be\n",
    "            a single item, not a batch, and it will be replicated batch_size times.\n",
    "        batch_size (int): the batch size\n",
    "        use_amp (bool): whether to estimate based on using mixed precision\n",
    "        device (torch.device): the device to use\n",
    "    \"\"\"\n",
    "    # Reset model and optimizer\n",
    "    model.cpu()\n",
    "    a = torch.cuda.memory_allocated(device)\n",
    "    model.to(device)\n",
    "    b = torch.cuda.memory_allocated(device)\n",
    "    model_memory = b - a\n",
    "    model_input = sample_input  # .unsqueeze(0).repeat(batch_size, 1)\n",
    "    output = model(model_input.to(device)).sum()\n",
    "    total_memory = model_memory\n",
    "\n",
    "    return total_memory\n",
    "\n",
    "\n",
    "estimate_memory_inference(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "inputs = torch.randn(1, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torch.nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=20, T_mult=2, eta_min=0.001, last_epoch=-1\n",
    ")\n",
    "\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    lr_sched.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
